# -*- coding: utf-8 -*-
"""Copy of SentimentLogistic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_ubZu_LaUKBMyLBIgEUjfhFl8_Lms4qv
"""

import io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('words')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import warnings
from sklearn.metrics import accuracy_score, classification_report
import pickle

from google.colab import files
uploaded = files.upload()

df = pd.read_csv(io.BytesIO(uploaded['data.csv']), header=0, index_col=0, sep=',',lineterminator='\r',encoding='latin-1')
df.head()

#Function to process/clean the tweets
pun = """!"$%&'()*+,-./:;<=>?[\]^`{|}~"""
stop_words = set(stopwords.words('english'))
lem = WordNetLemmatizer()


def clean_text(words):
    """The function to clean text"""
    words = re.sub("[^a-zA-Z]", " ", words)
    
    # Remove urls
    text = re.sub(r"http\S+|www\S+|https\S+", '', words, flags=re.MULTILINE)
    # Remove user @ references and '#' from tweet
    text = re.sub(r'\@\w+','', text)
    text = re.sub(r'\#\w+','', text)
    #remove numbers    
    #digits = '[0-9]'
    #text = re.sub(digits, '', text)
    #Remove emojis
    text = re.sub('(?::|;|=)(?:-)?(?:\)|\(|D|P)'," ",text)
    text = text.lower().split()
    return " ".join(text)

def remove_numbers(text):
    """The function to removing all numbers"""
    new_text = []
    for word in text.split():
        if not re.search('\d', word):
            new_text.append(word)
    return ' '.join(new_text)

def remove_stopwords(review):
    """The function to removing stopwords"""
    text = [word.lower() for word in review.split() if word.lower() not in stop_words]
    return " ".join(text)

def get_lemmatize(text):
    """The function to apply lemmatizing"""
    lem_text = [lem.lemmatize(word) for word in text.split()]
    return " ".join(lem_text)

print(df.shape)

# Preprocess the data

# removes pattern in the input text
def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for word in r:
        input_txt = re.sub(word, "", input_txt)
    return input_txt

# remove twitter handles (@user)
df['tweet'] = df['tweet'].astype(str)
df['tweet'] = df['tweet'].apply(clean_text)
print("done")
df['tweet'] = df['tweet'].apply(remove_stopwords)
df['tweet'] = df['tweet'].apply(remove_numbers)
df['tweet'] = df['tweet'].apply(get_lemmatize)
#df['clean_tweet'] = np.vectorize(remove_pattern)(df['tweet'], "@[\w]*")

# remove special characters, numbers and punctuations
#df['clean_tweet'] = df['clean_tweet'].str.replace("[^a-zA-Z#]", " ")

# remove short words
#df['clean_tweet'] = df['clean_tweet'].apply(lambda x: " ".join([w for w in x.split() if len(w)>3]))

#tokenized_tweet = df['clean_tweet'].apply(lambda x: x.split())

# stem the words
#from nltk.stem.porter import PorterStemmer
#stemmer = PorterStemmer()

#tokenized_tweet = tokenized_tweet.apply(lambda sentence: [stemmer.stem(word) for word in sentence])


# combine words into single sentence
#for i in range(len(tokenized_tweet)):
    #tokenized_tweet[i] = " ".join(tokenized_tweet[i])
    
#df['clean_tweet'] = tokenized_tweet
#df.head()

with open('data_clean.csv', 'w') as f:
  df.to_csv(f)

dataset = pd.read_csv('data_clean.csv', header=0, index_col=0)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
# Shape

print(dataset.shape)

# Separate into input and output columns
X = dataset['tweet'].values.astype('U')
y = dataset['label'].values.astype('U')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)
model = Pipeline([('vect', CountVectorizer(min_df=5, ngram_range=(1, 2))),
                  ('tfidf', TfidfTransformer()),
                  ('model', LogisticRegression()), ])
# training
#model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions
ytest = np.array(y_test)
pred_y = model.predict(X_test)

# Evaluate predictions
print('accuracy %s' % accuracy_score(pred_y, y_test))
print(classification_report(ytest, pred_y))

# Save the model
with open("sentiment.pkl", "wb") as f:
    pickle.dump(model,f)

# testing
with open('sentiment.pkl', 'rb') as f:
        model = pickle.load(f)
s=['Abortion is bad']
a=model.predict(s)
print(a[0])

